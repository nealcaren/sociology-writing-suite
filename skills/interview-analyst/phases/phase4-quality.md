# Phase 4: Quality Checkpoint

You are executing Phase 4 of a pragmatic qualitative analysis. Your goal is to **evaluate the analysis so far** against established quality indicators and identify what needs strengthening before synthesis.

## The Logic of Quality Assessment

Quality in qualitative research isn't about replicating quantitative standards—it requires its own criteria. Drawing on Small & Calarco's *Qualitative Literacy*, this phase assesses the analysis against five indicators that mark high-quality interview research.

This is a **checkpoint, not a gate**. The goal is to identify strengths and weaknesses so they can be addressed, not to pass/fail the analysis.

## Inputs

Before starting, read ALL prior phase outputs:
1. `/analysis/phase1-reports/` - Immersion and familiarization
2. `/analysis/phase2-reports/` - Coding
3. `/analysis/phase3-reports/` - Interpretation and explanation
4. Original transcripts in `/interviews/` as needed

## The Five Quality Indicators

### 1. Cognitive Empathy

**The standard**: Has the analysis come to understand participants as they understand themselves?

Cognitive empathy has three dimensions:
- **Perception**: How do participants perceive their situation, their world?
- **Meaning**: What do things mean to them? How do they interpret events?
- **Motivation**: What drives their actions? What do they want, fear, hope?

**Assessment questions**:
- Do the interview memos and analysis capture how participants see things?
- Is there evidence of understanding participants' perspectives from the inside?
- Can a reader grasp what the experience is like for participants?
- Have we avoided reducing participants' views to "false consciousness" or dismissing them?
- Have we also avoided simply taking accounts at face value without critical analysis?

**Red flags**:
- Analysis that speaks primarily in the analyst's voice without participant perspective
- Claims about what participants think/feel without supporting evidence
- Dismissive treatment of participant accounts
- Failure to consider how participants understand their own situations

### 2. Heterogeneity

**The standard**: Has the analysis captured variation—within individuals, across the sample, and across contexts?

**Dimensions to assess**:
- **Within individuals**: Do we see participants as complex, potentially contradictory, changing over time?
- **Across the sample**: Have we represented the diversity of experiences, perspectives, and trajectories?
- **Across contexts**: Where relevant, have we captured variation across settings or circumstances?

**Assessment questions**:
- Does the analysis acknowledge differences across participants, or does it homogenize?
- Are negative cases and outliers addressed, not just ignored?
- Do we see individuals as complex people, not one-dimensional types?
- Is variation treated as analytically interesting, not just noise?

**Red flags**:
- Treating the sample as uniform ("Participants said...")
- Ignoring or minimizing cases that don't fit the main pattern
- Presenting participants as simple embodiments of a type
- Failing to examine what explains differences

### 3. Palpability

**The standard**: Is the evidence concrete and specific, not abstract and general?

Palpable evidence lets readers see what actually happened—specific events, actual words, real situations. Abstract evidence tells readers what to conclude without showing them the basis.

**Assessment questions**:
- Do quotes capture specific moments, events, situations—not just general attitudes?
- Is there enough context to understand what quotes mean?
- Can readers "see" what we're describing?
- Do claims rest on concrete evidence or general impressions?

**Red flags**:
- Over-reliance on general summary statements
- Quotes that express attitudes rather than describe experiences
- Claims made without specific evidentiary support
- Evidence that feels thin or cherry-picked

### 4. Follow-Up

**The standard**: Has the analysis pursued emergent leads and probed beneath surface responses?

Good interviewing and analysis involves following up—not accepting initial responses as complete, but probing for specifics, examples, elaboration.

**Assessment questions**:
- Is there evidence of probing beneath surface accounts?
- Have emergent themes been pursued across interviews?
- Are there gaps in the evidence that could have been filled with follow-up?
- Does the analysis go beyond what any single interview reveals?

**Red flags**:
- Taking initial responses at face value without probing
- Ignoring interesting leads that emerge during analysis
- Gaps in evidence that sharper interviewing might have filled
- Relying too heavily on general statements rather than specific examples

### 5. Self-Awareness

**The standard**: Has there been reflexivity about the analyst's position, assumptions, and potential biases?

**Domains to assess**:
- **Positionality**: How might the analyst's social position affect what they see/miss?
- **Assumptions**: What assumptions have guided the analysis? Have they been examined?
- **Limitations**: What are the boundaries of what this analysis can claim?

**Assessment questions**:
- Is there acknowledgment of the analyst's perspective?
- Have assumptions been made explicit and examined?
- Are limitations honestly addressed?
- Is there awareness of what this data/analysis cannot show?

**Red flags**:
- Invisible analyst (claims presented as objective truth)
- Unexamined assumptions driving interpretation
- Overclaiming (asserting more than evidence supports)
- Failure to acknowledge limitations

## Your Tasks

### 1. Assess Each Indicator

For each of the five indicators, evaluate the analysis:

```markdown
## [Indicator Name]

**Strengths**:
- [What the analysis does well on this dimension]

**Weaknesses**:
- [Where the analysis falls short]

**Specific examples**:
- [Evidence from the analysis illustrating strengths or weaknesses]

**Recommendations**:
- [What could be done to strengthen this dimension]

**Rating**: Strong / Adequate / Needs Improvement
```

### 2. Identify Critical Gaps

Based on your assessment:
- What are the most significant quality concerns?
- Which gaps would most undermine the analysis if unaddressed?
- What must be done before proceeding to synthesis?

### 3. Recommend Remediation

For each significant weakness, recommend specific actions:
- Return to transcripts for additional coding?
- Reread with attention to specific dimensions?
- Revise interpretations?
- Acknowledge limitations?
- Strengthen evidentiary basis?

### 4. Note Strengths

Also document what the analysis does well—this helps maintain what's working and provides language for methods sections.

## Output Files to Create

Save all outputs to `/analysis/phase4-reports/`:

1. **cognitive-empathy-assessment.md** - Evaluation of indicator 1
2. **heterogeneity-assessment.md** - Evaluation of indicator 2
3. **palpability-assessment.md** - Evaluation of indicator 3
4. **follow-up-assessment.md** - Evaluation of indicator 4
5. **self-awareness-assessment.md** - Evaluation of indicator 5
6. **critical-gaps.md** - Most significant concerns and required remediation
7. **strengths-summary.md** - What the analysis does well
8. **phase4-report.md** - Executive summary including:
   - Overview assessment (overall quality rating)
   - Summary of each indicator
   - Critical gaps requiring attention
   - Recommended remediation actions
   - Strengths to preserve
   - Questions for the user
   - Recommendation: proceed to Phase 5 or return to earlier phase?

## Assessment Scale

For each indicator:

**Strong**: Clearly meets the standard. Evidence of careful attention to this dimension throughout.

**Adequate**: Generally meets the standard with minor weaknesses. Some room for improvement but not critically flawed.

**Needs Improvement**: Significant gaps. Should be addressed before finalizing analysis.

## Guiding Principles

1. **Be honest, not harsh**: The goal is improvement, not judgment. Identify issues clearly but constructively.

2. **Be specific**: "Needs more palpability" is less useful than "Claims about recruitment patterns lack specific quotes showing how invitation conversations actually happened."

3. **Distinguish critical from minor**: Some gaps undermine the analysis; others are minor imperfections. Focus energy on what matters most.

4. **Consider the research questions**: Quality assessment should be calibrated to what the study aims to accomplish.

5. **Improvement is always possible**: Even strong work can be strengthened. Even weak work can be salvaged.

## Example Assessment

```markdown
## Palpability Assessment

**Strengths**:
- Turning point narratives are richly detailed with specific quotes
- P07 and P11's accounts of recruitment are particularly vivid
- The coding memo provides concrete examples for most codes

**Weaknesses**:
- Claims about "relationship maintenance" lack specific examples of
  how participants actually maintain relationships (general statements
  about "staying in touch" but no descriptions of specific interactions)
- Evidence for "cost-benefit calculations" is thin—mostly analyst
  inference rather than participant accounts
- Section on organizational dynamics relies on summary rather than
  showing actual meeting interactions

**Specific examples**:
- Strong: "The quote from P03 about 'the moment everything changed'
  lets readers see exactly what happened at that rally."
- Weak: "The claim that 'participants weighed costs and benefits'
  (Phase 3 report, p.7) has no supporting quotes showing this process."

**Recommendations**:
- Return to transcripts to find specific examples of relationship
  maintenance practices
- Either find better evidence for cost-benefit framing or revise
  interpretation
- Add concrete quotes to organizational dynamics section

**Rating**: Adequate (strong in some areas, but gaps in evidence
for key claims need addressing)
```

## When You're Done

Return a summary to the orchestrator that includes:
1. Confirmation that all files were created
2. Overall quality assessment
3. Rating for each of the five indicators
4. Most critical gaps (1-3 that must be addressed)
5. Key strengths to preserve
6. Recommended next steps:
   - Proceed to Phase 5?
   - Return to earlier phase for additional work?
   - Specific remediation tasks?
7. Questions for the user about priorities
